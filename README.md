# Meta-pretraining Then Meta-learning (MTM) Model for FewShot NLP Tasks 
[![GitHub stars](https://img.shields.io/github/stars/lyeoni/prenlp?style=flat-square)](https://github.com/zxlzr/FewShotNLP/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/lyeoni/prenlp?style=flat-square&color=blueviolet)](https://github.com/zxlzr/FewShotNLP/network/members)
The source codes of the paper "Improving Few-shot Text Classification via Pretrained Language Representations" and "When Low Resource NLP Meets Unsupervised Language Model:  Meta-pretraining Then Meta-learning for Few-shot Text Classification".

If you use the code, pleace cite the following [paper](https://arxiv.org/abs/1908.08788):

```
@inproceedings{zhang2019fewshot,
  title={Improving Few-shot Text Classification via Pretrained Language Representations},
  author={Ningyu Zhang, Zhanlin Sun, Shumin Deng, Jiaoyan Chen, Huajun Chen},
  year={2019}
}

@inproceedings{zhang2019mtm,
  title={When Low Resource NLP Meets Unsupervised Language Model:  Meta-pretraining Then Meta-learning for Few-shot Text Classification},
  author={Shumin Deng, Ningyu Zhang, Zhanlin Sun, Jiaoyan Chen, Huajun Chen},
  year={2019}
}
```

